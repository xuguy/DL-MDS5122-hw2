{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11404510,"sourceType":"datasetVersion","datasetId":7005374}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport sys\nimport re\nimport torch.nn as nn\nfrom torch.nn import DataParallel\n# from torch.nn import TransformerDecoder, TransformerDecoderLayer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device.type == 'cpu':\n    file_path = './kaggleData/data/eng-cmn.txt'\nelse:\n    file_path = '/kaggle/input/eng-cmn/eng-cmn.txt'\n    sys.path.append('/kaggle/input/eng-cmn')\n\n\ntext_pairs = []\nwith open(file_path, 'r', encoding='utf-8') as f:\n    for line in f:\n\n        parts = line.strip().split('\\t')\n        \n\n        if len(parts) < 3:\n            continue\n            \n        en_text = parts[0].strip()\n        zh_text = parts[1].strip()\n        \n\n        if en_text and zh_text:\n\n            text_pairs.append((zh_text, en_text))\n\ndef split_chinese_english(s):\n\n    pattern = re.compile(r'([\\u4e00-\\u9fff]+)|([^\\u4e00-\\u9fff]+)')\n    result = []\n    for block in pattern.findall(s):\n        chinese_part, non_chinese_part = block\n        if chinese_part:\n            \n            result.extend(list(chinese_part))\n        elif non_chinese_part:\n\n            subparts = non_chinese_part.lower().split()\n            for sub in subparts:\n\n                processed = re.sub(r'[^a-zA-Z]', '', sub)\n                if processed:\n                    result.append(processed)\n    return result\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T04:14:54.770145Z","iopub.execute_input":"2025-04-18T04:14:54.770425Z","iopub.status.idle":"2025-04-18T04:14:54.835066Z","shell.execute_reply.started":"2025-04-18T04:14:54.770406Z","shell.execute_reply":"2025-04-18T04:14:54.834587Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from collections import defaultdict\n\ndef build_vocab(text_pairs, unknown = True):\n    '''\n    text_pairs: raw cn-eng text pair\n    unknown: whether to deal with unkonwn words\n    '''\n    vocab = defaultdict(lambda: len(vocab))\n    special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<sep>\"]\n    for token in special_tokens:\n        vocab[token]\n\n    for ch, en in text_pairs:\n        ch = split_chinese_english(ch)\n        for char in ch:\n            vocab[char]\n        en = split_chinese_english(en)\n        for char in en:\n            vocab[char]\n    # add <unk> to the las of the vocab to deal with unknown words\n    if unknown:    \n        vocab['<unk>']\n    \n    # freeze vocab (check usage of 'defaultdict')\n    vocab.default_factory = None\n    \n    return vocab\n\ndef text_to_ids(text, vocab):\n    unk_id = vocab['<unk>']\n    split = split_chinese_english(text)\n    # print(split)\n    ids = [vocab.get(id, unk_id) for id in split]\n    return ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T04:14:59.807345Z","iopub.execute_input":"2025-04-18T04:14:59.807613Z","iopub.status.idle":"2025-04-18T04:14:59.813899Z","shell.execute_reply.started":"2025-04-18T04:14:59.807593Z","shell.execute_reply":"2025-04-18T04:14:59.813016Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# build vocab\nvocab = build_vocab(text_pairs)\nlen(vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T04:15:15.569036Z","iopub.execute_input":"2025-04-18T04:15:15.569659Z","iopub.status.idle":"2025-04-18T04:15:15.846307Z","shell.execute_reply.started":"2025-04-18T04:15:15.569635Z","shell.execute_reply":"2025-04-18T04:15:15.845750Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"10738"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# build dataset\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass TranslationDataset(Dataset):\n    def __init__(self, pairs, vocab, max_len=50):\n        self.pairs = pairs\n        self.vocab = vocab\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        ch, en = self.pairs[idx]\n\n        input_ids_cn_eng = (\n            [self.vocab[\"<sos>\"]] +\n            text_to_ids(ch, self.vocab) +\n            [self.vocab[\"<sep>\"]] +\n            text_to_ids(en, self.vocab) +\n            [self.vocab[\"<eos>\"]]\n        )\n\n        input_ids_cn_eng = input_ids_cn_eng[:self.max_len]\n        src = input_ids_cn_eng[:-1]\n        tgt = input_ids_cn_eng[1:]\n\n        # pad to max_len\n        # ================================================================\n        src += [self.vocab[\"<pad>\"]]* (self.max_len - len(src))\n        tgt += [self.vocab[\"<pad>\"]]* (self.max_len - len(tgt))\n        # ================================================================\n\n        return torch.tensor(src), torch.tensor(tgt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T04:15:20.254946Z","iopub.execute_input":"2025-04-18T04:15:20.255664Z","iopub.status.idle":"2025-04-18T04:15:20.261274Z","shell.execute_reply.started":"2025-04-18T04:15:20.255640Z","shell.execute_reply":"2025-04-18T04:15:20.260679Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# hyper params\n\nvocab_size = len(vocab)\neval_num = 100\nBATCH_SIZE = 32\nMAX_LEN = 49\nmodel_max_seq=128\nmax_epoch = 4\nnum_heads = 2\n\ndataset = TranslationDataset(text_pairs, vocab, MAX_LEN)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T04:30:31.934190Z","iopub.execute_input":"2025-04-18T04:30:31.934960Z","iopub.status.idle":"2025-04-18T04:30:31.938934Z","shell.execute_reply.started":"2025-04-18T04:30:31.934936Z","shell.execute_reply":"2025-04-18T04:30:31.938259Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# define a real gpt:\nfrom BuildingBlocks import MultiHeadAttention\n\n# gpt config:\nGPT_CONFIG_124M = {\n\"vocab_size\": len(vocab), # Vocabulary size\n\"context_length\": 128, # Context length: max sequence length the model can handle \n\"emb_dim\": 64, # Embedding dimension\n\"n_heads\": 4, # Number of attention heads\n\"n_layers\": 4, # Number of layers\n\"drop_rate\": 0.1, # Dropout rate\n\"qkv_bias\": False # Query-Key-Value bias\n}\n\nclass LayerNorm(nn.Module):\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n\n    def forward(self, x):\n        mean = x.mean(dim = -1, keepdim = True)\n        # unbiased=False: divided by n instead of n-1\n        var = x.var(dim = -1, keepdim = True, unbiased = False)\n        norm_x = (x-mean) / torch.sqrt(var + self.eps)\n        return self.scale * norm_x + self.shift\n    \nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5*x*(1+torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 *torch.pow(x, 3))\n        ))\n    \nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            # (2, 3, 768)->(2, 3, 3072)\n            nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n            # (2,3,3072) -> (2,3,3072)\n            GELU(),\n            # (2,3,3072)->(2,3,768)\n            nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n        )\n    def forward(self, x):\n        return self.layers(x)\n    \nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention(\n            d_in = cfg[\"emb_dim\"],\n            d_out = cfg[\"emb_dim\"],\n            context_length=cfg[\"context_length\"],\n            num_heads = cfg[\"n_heads\"],\n            dropout = cfg[\"drop_rate\"],\n            qkv_bias = cfg[\"qkv_bias\"])\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n    def forward(self, x):\n\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut\n\n        shortcut = x\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut\n        return x\n    \n# GPT DEF\nclass GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n        )\n\n    def forward(self, in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(\n            torch.arange(seq_len, device = in_idx.device)\n        )\n\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T04:30:35.531874Z","iopub.execute_input":"2025-04-18T04:30:35.532398Z","iopub.status.idle":"2025-04-18T04:30:35.544746Z","shell.execute_reply.started":"2025-04-18T04:30:35.532376Z","shell.execute_reply":"2025-04-18T04:30:35.544110Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T04:33:08.802397Z","iopub.execute_input":"2025-04-18T04:33:08.803355Z","iopub.status.idle":"2025-04-18T04:33:08.809392Z","shell.execute_reply.started":"2025-04-18T04:33:08.803319Z","shell.execute_reply":"2025-04-18T04:33:08.808334Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"1744950788.8046644"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"\n# initialize model and start training\n\n# model = SimpleGPTTranslator(vocab_size=vocab_size).to(device)\nmodel = GPTModel(GPT_CONFIG_124M).to(device)\n\nK = torch.cuda.device_count()\n\nif K > 1:\n    print(f'======= training in {K} GPUs =======')\n    model = DataParallel(model, device_ids = list(range(K)))\n    \n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n# criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\ncriterion = nn.CrossEntropyLoss(ignore_index = -100)\nignore_index = [0] # ignore <pad>\n\n\n# training loop cn-eng:\nfor epoch in range(max_epoch):\n    total_loss = 0\n    start = time.time()\n    for src, tgt in dataloader:\n        \n        src = src.to(device)\n        tgt = tgt.to(device)\n        # print(tgt)\n        msk = torch.isin(tgt, torch.tensor(ignore_index).to(device))\n        tgt[msk] = -100\n        # print(tgt)\n        # print(src.shape)\n        optimizer.zero_grad()\n        # print(src)\n        \n        output = model(src)\n        # output = model_dp(src)\n        \n        # print(output.shape)\n        loss = criterion(output.view(-1, vocab_size), tgt.view(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    end = time.time()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}, Time: {end-start:.4f}s\")\n\n# # training loop reverse: eng-cn\n# for epoch in range(max_epoch):\n#     total_loss = 0\n#     for src, tgt in dataloaderreverse:\n#         src = src.to(device)\n#         tgt = tgt.to(device)\n#         # print(tgt)\n#         msk = torch.isin(tgt, torch.tensor(ignore_index).to(device))\n#         tgt[msk] = -100\n#         # print(tgt)\n#         # print(src.shape)\n#         optimizer.zero_grad()\n#         # print(src)\n#         output = model(src)\n#         # print(output.shape)\n#         loss = criterion(output.view(-1, vocab_size), tgt.view(-1))\n#         loss.backward()\n#         optimizer.step()\n#         total_loss += loss.item()\n#     print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n\n# save model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T04:53:06.764714Z","iopub.execute_input":"2025-04-18T04:53:06.765478Z","iopub.status.idle":"2025-04-18T04:53:49.419418Z","shell.execute_reply.started":"2025-04-18T04:53:06.765449Z","shell.execute_reply":"2025-04-18T04:53:49.418488Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 5.3599, Time: 15.8013s\nEpoch 2, Loss: 4.3952, Time: 15.7679s\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2988269710.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m# print(tgt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mmsk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":35},{"cell_type":"code","source":"# save model\ntorch.save(model.module.state_dict(), \"gpt2_rev_weights.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T04:41:24.129923Z","iopub.execute_input":"2025-04-18T04:41:24.130554Z","iopub.status.idle":"2025-04-18T04:41:24.155939Z","shell.execute_reply.started":"2025-04-18T04:41:24.130531Z","shell.execute_reply":"2025-04-18T04:41:24.155413Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# reload model\n# model = SimpleGPTTranslator(vocab_size=vocab_size).to(device)\nmodel = GPTModel(GPT_CONFIG_124M).to(device)\nmodel = DataParallel(model, device_ids = list(range(K)))\nmodel.module.load_state_dict(torch.load('gpt2_rev_weights.pth', map_location=device, weights_only=True))\n# model.load_state_dict(torch.load('gpt2_weights.pth', map_location=device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T04:46:55.861850Z","iopub.execute_input":"2025-04-18T04:46:55.862373Z","iopub.status.idle":"2025-04-18T04:46:55.907600Z","shell.execute_reply.started":"2025-04-18T04:46:55.862350Z","shell.execute_reply":"2025-04-18T04:46:55.907057Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# translate chinese to english\ndef translate_cn_eng(model, ch_text, vocab, max_len=80):\n    model.to(device)\n    model.eval()\n\n    input_ids = (\n        [vocab[\"<sos>\"]] +\n        text_to_ids(ch_text, vocab) +\n        [vocab[\"<sep>\"]]\n    )\n    input_tensor = torch.tensor([input_ids], device=device)\n\n\n    for _ in range(max_len):\n        if len(input_ids) >50:\n            # print('max length hit')\n            break\n        output = model(input_tensor)\n\n        # extract the last words of output as the predicted words\n        # [0, -1]: 0 for batch, since we only have 1 data per batch\n        next_token = output[0, -1].argmax().item()\n        # print(next_token)\n        input_ids.append(next_token)\n        if next_token == vocab[\"<eos>\"]:\n            print('predict ends')\n            break\n        \n\n        input_tensor = torch.tensor([input_ids], device=device)\n    \n\n    sep_pos = input_ids.index(vocab[\"<sep>\"])\n    en_ids = input_ids[sep_pos+1:-1]\n    \n\n    id_to_token = {v: k for k, v in vocab.items()}\n    return \" \".join([id_to_token[id] for id in en_ids])\n\n\n# translate english to chinese (deprecated)\ndef translate_eng_cn(model, eng_text, vocab, max_len=80):\n    model.to(device)\n    model.eval()\n\n    input_ids = (\n        [vocab[\"<sos>\"]] +\n        text_to_ids(eng_text, vocab) +\n        [vocab[\"<sep>\"]]\n    )\n    input_tensor = torch.tensor([input_ids], device=device)\n    # print(input_tensor.device)\n    # input_tensor = input_tensor.to(device)\n    \n\n    for _ in range(max_len):\n        if len(input_ids) >50:\n            # print('max length hit')\n            break\n        output = model(input_tensor)\n\n        # extract the last words of output as the predicted words\n        # [0, -1]: 0 for batch, since we only have 1 data per batch\n        next_token = output[0, -1].argmax().item()\n        # print(next_token)\n        input_ids.append(next_token)\n        if next_token == vocab[\"<eos>\"]:\n            print('predict ends')\n            break\n        \n\n        input_tensor = torch.tensor([input_ids], device=device)\n    \n\n    sep_pos = input_ids.index(vocab[\"<sep>\"])\n    ch_ids = input_ids[sep_pos+1:-1]\n    \n\n    id_to_token = {v: k for k, v in vocab.items()}\n    return \"\".join([id_to_token[id] for id in ch_ids])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T04:43:45.430364Z","iopub.execute_input":"2025-04-18T04:43:45.430664Z","iopub.status.idle":"2025-04-18T04:43:45.439492Z","shell.execute_reply.started":"2025-04-18T04:43:45.430644Z","shell.execute_reply":"2025-04-18T04:43:45.438714Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"translate_cn_eng(model, '牛', vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T04:47:02.292766Z","iopub.execute_input":"2025-04-18T04:47:02.293376Z","iopub.status.idle":"2025-04-18T04:47:02.329299Z","shell.execute_reply.started":"2025-04-18T04:47:02.293353Z","shell.execute_reply":"2025-04-18T04:47:02.328575Z"}},"outputs":[{"name":"stdout","text":"predict ends\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"'how much'"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# eval\n# chinese to english\nimport numpy as np\nfor i in np.random.randint(0, len(text_pairs), eval_num):\n    cn = text_pairs[i][0]\n    en = text_pairs[i][1]\n    print(f'> {cn}')\n    print(f'= {en}')\n    print(f'< {translate_cn_eng(model, cn, vocab)}')\n    print('='*20)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T04:44:43.065284Z","iopub.execute_input":"2025-04-18T04:44:43.065557Z","iopub.status.idle":"2025-04-18T04:44:48.117097Z","shell.execute_reply.started":"2025-04-18T04:44:43.065539Z","shell.execute_reply":"2025-04-18T04:44:48.116484Z"}},"outputs":[{"name":"stdout","text":"> 这是他们应得的。\n= They deserve it.\npredict ends\n< this is the same\n====================\n> 乡间有很多树。\n= There are lots of trees in the countryside.\npredict ends\n< there are many books\n====================\n> 據說她愛他。\n= It's said that she loves him.\npredict ends\n< he knows him\n====================\n> 你去哪儿？\n= Where are you going?\npredict ends\n< where are you going\n====================\n> 火車上有預訂的座位嗎？\n= Are there reserved seats on the train?\npredict ends\n< is the desk in the world\n====================\n> 他們那麼年輕。\n= They were so young.\npredict ends\n< they are they\n====================\n> 我们不能仅仅就这样炒了汤姆\n= We can't just fire Tom.\npredict ends\n< we cant tell tom\n====================\n> 我不是在罵你。\n= I was not scolding you.\npredict ends\n< i dont have you\n====================\n> 这汤里盐放多了。\n= There's too much salt in this soup.\npredict ends\n< how much did tom\n====================\n> 您会使用计算机吗？\n= Can you use a computer?\npredict ends\n< do you want to buy a little more coffee\n====================\n> 湯姆讓瑪麗回家。\n= Tom let Mary go home.\npredict ends\n< tom went to go home\n====================\n> 我一到伦敦就会给你留言的。\n= As soon as I get to London, I'll drop you a line.\npredict ends\n< i wish you can help the same age\n====================\n> 我把我借来的刀还了。\n= I returned the knife which I had borrowed.\npredict ends\n< i bought my house is my house\n====================\n> 车站前有个银行。\n= There is a bank in front of the station.\npredict ends\n< there are a car of the train\n====================\n> 下一个给你。\n= The next one's for you.\npredict ends\n< turn you\n====================\n> 你需要什麼?\n= What do you need?\npredict ends\n< what do you need\n====================\n> 湯姆是最先作出反應的。\n= Tom was the first to react.\npredict ends\n< tom is a good driver\n====================\n> 我們不知道下一步要做什麼。\n= We didn't know what to do next.\npredict ends\n< what do you know what to do\n====================\n> 这部电影至少值得看两三遍。\n= The movie is worth seeing at least two or three times.\npredict ends\n< this book is a new york\n====================\n> 這是什麼地方?\n= Where am I?\npredict ends\n< whats this\n====================\n> 我们知道我们的权利。\n= We know our rights.\npredict ends\n< we know what we know\n====================\n> 現在長裙不流行了。\n= Long skirts are out of fashion now.\npredict ends\n< its not been here\n====================\n> 你认识他吗？\n= Do you know him?\npredict ends\n< do you think he say\n====================\n> 我的假期一下就過完了。\n= My vacation went by quickly.\npredict ends\n< my homework is already finished\n====================\n> 請給我一杯牛奶。\n= Please give me a glass of milk.\npredict ends\n< please give me a cup of paper\n====================\n> 什么时候开始？\n= When will it begin?\npredict ends\n< when will you go\n====================\n> 我們的看法有一點不同。\n= We had a slight difference of opinion.\npredict ends\n< we have a little more time\n====================\n> 汤姆说它是钻石。\n= Tom said that it was a diamond.\npredict ends\n< tom said that\n====================\n> 我们今晚出去吃怎么样？\n= How about dining out tonight?\npredict ends\n< how we go to eat today\n====================\n> 这两个男人是生意上的合作伙伴。\n= The two men were business partners.\npredict ends\n< this is a famous of the boy\n====================\n> 你必須做選擇。\n= You have to choose.\npredict ends\n< you need to do\n====================\n> 他们互帮互助，把校庆搞得很成功。\n= They helped one another to make the school festival a success.\npredict ends\n< they should have a lot of the police\n====================\n> 六除以二得三。\n= Six divided by two equals three.\npredict ends\n< the book is very much\n====================\n> 她是个数学天才。\n= She was a genius in mathematics.\npredict ends\n< she is a school\n====================\n> 她馬上離開了這裡。\n= She left here right away.\npredict ends\n< she got here to the station\n====================\n> 我知道你知道我知道。\n= I know that you know that I know.\npredict ends\n< i know what you know\n====================\n> 他常常走路去學校。\n= He often walks to school.\npredict ends\n< he often go to school to school\n====================\n> 我不喜歡你和我說話的口氣。\n= I don't like the way you talk to me.\npredict ends\n< i dont like you like to me\n====================\n> 請把鹽遞給我，好嗎？\n= Pass me the salt, will you?\npredict ends\n< please give me please\n====================\n> 他想在醫院工作。\n= He wants to work in a hospital.\npredict ends\n< he wants to the bus\n====================\n> 湯姆以前是教師嗎？\n= Did Tom use to be a teacher?\npredict ends\n< is tom and mary\n====================\n> 不许大叫。\n= Don't shout.\npredict ends\n< dont have a good\n====================\n> 他們在小學時就是朋友了。\n= They became friends in elementary school.\npredict ends\n< they were a student of people\n====================\n> 我希望如此。\n= I hope so.\npredict ends\n< i hope\n====================\n> 可能你工作得太拼命？\n= Maybe you're working too hard.\npredict ends\n< can you get your phone\n====================\n> 那只黑鸟不是乌鸫。\n= That black bird is not a blackbird.\npredict ends\n< thats not a new york\n====================\n> 那太好了，不是嗎？\n= That's good, isn't it?\npredict ends\n< is it isnt it\n====================\n> 人人生来平等。\n= All men are created equal.\npredict ends\n< he is a doctor\n====================\n> 一切都變成了黑色。\n= Everything went black.\npredict ends\n< the boy was filled the station\n====================\n> 你被不公平地對待。\n= You were treated unfairly.\npredict ends\n< you cant get up with your health\n====================\n> 她十幾歲時就結婚了。\n= She got married in her teens.\npredict ends\n< she got married every morning\n====================\n> 这个男孩子很懒。\n= This boy is lazy.\npredict ends\n< this boy is very much\n====================\n> 下一班火車什麼時候開？\n= When does the next train leave?\npredict ends\n< when did you go to the train\n====================\n> 你能稍微告诉我关于自己的事情吗？\n= Can you tell me a little about yourself?\npredict ends\n< can you tell me to tell me\n====================\n> 我想要的就是这个。\n= It's exactly what I wanted.\npredict ends\n< i want to do this\n====================\n> 这附近有没有可以给手机充电的地方？\n= Is there a place where I can charge my cellphone around here?\npredict ends\n< this company is not a new york\n====================\n> 巴士站離我們的學校近。\n= The bus stop is near our school.\npredict ends\n< the school was busy well\n====================\n> 有一天，我会跑得像风一样快。\n= Someday I'll run like the wind.\npredict ends\n< i can be a little more time\n====================\n> 老闆已經批准了我們的計劃。\n= The boss has already approved of our plans.\npredict ends\n< the boy was filled our teacher\n====================\n> 他喜欢地理和历史。\n= He likes geography and history.\npredict ends\n< he likes to the same age\n====================\n> 既要马好，又要马不吃草。\n= You can't have your cake and eat it, too.\npredict ends\n< dont look to eat to eat\n====================\n> 我沒做什麼與眾不同的事。\n= I did nothing out of the ordinary.\npredict ends\n< i dont do anything\n====================\n> 湯姆說他想哭。\n= Tom said that he wanted to cry.\npredict ends\n< tom wanted to know him\n====================\n> 我們參加了討論。\n= We took part in the discussion.\npredict ends\n< we got to the bus\n====================\n> 一般而言，日本人很害羞。\n= Generally, Japanese people are shy.\npredict ends\n< a lot of the boy is very much\n====================\n> 你昨晚做了什麼？\n= What did you do last night?\npredict ends\n< what did you do\n====================\n> 她拒絕接受這筆錢。\n= She refused to accept the money.\npredict ends\n< she was the same age\n====================\n> 他们进攻了敌人。\n= They attacked the enemy.\npredict ends\n< they went to the train\n====================\n> 我有些事。\n= I have things to take care of.\npredict ends\n< i have a little\n====================\n> 我看我們贏不了。\n= I don't think we'd have any chance of winning.\npredict ends\n< i cant see us\n====================\n> 我那时没法去他的生日派对。\n= I wasn't able to go to his birthday party.\npredict ends\n< i didnt go to him in his job\n====================\n> 这是对的，不是吗？\n= That's right, isn't it?\npredict ends\n< is this\n====================\n> 汤姆没有时间。\n= Tom doesn't have time.\npredict ends\n< tom didnt have a little time\n====================\n> 不够吗？\n= Is that not enough?\npredict ends\n< dont you\n====================\n> 玛丽性格很好，就像她姐姐一样。\n= Mary has as attractive a personality as her sister.\npredict ends\n< mary is very good at her age\n====================\n> 他垒球打得很好。\n= He's very good at playing baseball.\npredict ends\n< he is very good at tennis\n====================\n> 我爱我的生命。\n= I love my life.\npredict ends\n< i love my fault\n====================\n> 那輛在樹下的自行車是我的。\n= The bicycle under the tree is mine.\npredict ends\n< this car is my eyes\n====================\n> 汤姆昨天用刀切到自己了。\n= Tom cut himself with his knife yesterday.\npredict ends\n< tom was already already already yesterday\n====================\n> 這個消息沒有道理。\n= This message doesn't make sense.\npredict ends\n< this is not be here\n====================\n> 别像猪一样猛吃东西。\n= Don't eat like a pig.\npredict ends\n< dont have a little more coffee\n====================\n> 你可以選擇他們之中的任何一個。\n= You may choose any of them.\npredict ends\n< you can have a lot of them\n====================\n> 我儿子不听我话。\n= My son doesn't obey me.\npredict ends\n< i dont think i am not trust me\n====================\n> 第二课容易。\n= Lesson Two is easy.\npredict ends\n< the accident was a cold\n====================\n> 一隻老虎逃出了動物園。\n= A tiger has escaped from the zoo.\npredict ends\n< a new dress and a new york\n====================\n> 湯姆說他想吃中國菜。\n= Tom said he wanted to eat Chinese food.\npredict ends\n< tom wants to buy him to eat\n====================\n> 我网球打得不那么好。\n= I don't play tennis that well.\npredict ends\n< i cant play tennis that\n====================\n> 你们什么时候开始学英语的？\n= When did you begin to learn English?\npredict ends\n< what do you study french\n====================\n> 她有三个孩子，信不信由你。\n= Believe it or not, she has three children.\npredict ends\n< she is not a girl of a child\n====================\n> 你不需要考试。\n= You don't have to take an examination.\npredict ends\n< you dont need to believe\n====================\n> 你可以借我一些錢嗎?\n= Would you lend me some money?\npredict ends\n< can you give me a book\n====================\n> 汤姆独自工作。\n= Tom works on his own.\npredict ends\n< tom works to work\n====================\n> 汤姆泪流满面。\n= Tom burst into tears.\npredict ends\n< tom is on the wall\n====================\n> 我很惊讶，你竟然不知道他们结婚了。\n= I'm surprised that you don't know about their marriage.\npredict ends\n< i know if you cant tell him\n====================\n> 穿上點衣服。\n= Put on some clothes.\npredict ends\n< the train is on the train\n====================\n> 我儿子十岁了。\n= My son is ten years old.\npredict ends\n< my house is a car\n====================\n> 我不干了。\n= I quit.\npredict ends\n< i dont have a cup\n====================\n> 他每天都打垒球。\n= He plays baseball every day.\npredict ends\n< he was very late last night\n====================\n> 感謝你的付出。\n= Thank you for everything.\npredict ends\n< thank you going to your health\n====================\n> 數千人死於飢餓。\n= Thousands of people died of hunger.\npredict ends\n< the car is full of the station\n====================\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# # english to chinese\n# import numpy as np\n# for i in np.random.randint(0, len(text_pairs), eval_num):\n#     cn = text_pairs[i][0]\n#     en = text_pairs[i][1]\n#     print(f'> {en}')\n#     print(f'= {cn}')\n#     print(f'< {translate_eng_cn(model, en, vocab)}')\n#     print('='*20)","metadata":{},"outputs":[],"execution_count":null}]}
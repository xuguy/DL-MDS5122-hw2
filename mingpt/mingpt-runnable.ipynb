{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./minGPT')\n",
    "from mingpt.model import GPT\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "# from mingpt.model import GPT\n",
    " \n",
    "# model_config = GPT.get_default_config()\n",
    "# model_config.vocab_size = len(vocab)  # 词汇表大小\n",
    "# model_config.block_size = 20          # 序列最大长度\n",
    "# model_config.n_layer = 6              # 参考网页6的层数配置\n",
    "# model_config.n_head = 8\n",
    "# model = GPT(model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\GITrepo\\DL-MDS5122-hw2\\mingpt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cpu':\n",
    "    file_path = '../kaggleData/data/eng-cmn.txt'\n",
    "else:\n",
    "    file_path = '/kaggle/input/eng-cmn/eng-cmn.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "\n",
    "        parts = line.strip().split('\\t')\n",
    "        \n",
    "\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "            \n",
    "        en_text = parts[0].strip()\n",
    "        zh_text = parts[1].strip()\n",
    "        \n",
    "\n",
    "        if en_text and zh_text:\n",
    "\n",
    "            text_pairs.append((zh_text, en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "# def build_vocab(text_pairs):\n",
    "#     vocab = defaultdict(lambda: len(vocab))\n",
    "#     special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<sep>\"]\n",
    "#     for token in special_tokens:\n",
    "#         vocab[token]\n",
    "    \n",
    "#     for ch, en in text_pairs:\n",
    "#         for char in (ch + en):\n",
    "#             vocab[char]\n",
    "#     return vocab\n",
    "def build_vocab(text_pairs):\n",
    "    vocab = defaultdict(lambda: len(vocab))\n",
    "    special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<sep>\"]\n",
    "    for token in special_tokens:\n",
    "        vocab[token]\n",
    "    \n",
    "    for ch, en in text_pairs:\n",
    "        ch = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', ch, flags=re.UNICODE)\n",
    "        for char in ch.lower():\n",
    "            vocab[char]\n",
    "        en = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', en, flags=re.UNICODE)\n",
    "        for char in en.lower().split(' '):\n",
    "            vocab[char]\n",
    "    return vocab\n",
    "\n",
    "def text_to_ids_cn(text, vocab, add_special_tokens=False):\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text, flags=re.UNICODE)\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text, flags=re.UNICODE)\n",
    "    text=text.lower()\n",
    "    ids = []\n",
    "    if add_special_tokens:\n",
    "        ids.append(vocab[\"<sos>\"])\n",
    "    for char in text:\n",
    "        ids.append(vocab[char])\n",
    "    # ids += [10, 2476, 2477, 85]\n",
    "    if add_special_tokens:\n",
    "        ids.append(vocab[\"<eos>\"])\n",
    "    return ids\n",
    "\n",
    "def text_to_ids_eng(text, vocab, add_special_tokens=False):\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text, flags=re.UNICODE)\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text, flags=re.UNICODE)\n",
    "    text=text.lower()\n",
    "    ids = []\n",
    "    if add_special_tokens:\n",
    "        ids.append(vocab[\"<sos>\"])\n",
    "    for char in text.split(' '):\n",
    "        ids.append(vocab[char])\n",
    "    \n",
    "    if add_special_tokens:\n",
    "        ids.append(vocab[\"<eos>\"])\n",
    "    return ids\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, vocab, max_len=50):\n",
    "        self.pairs = pairs\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ch, en = self.pairs[idx]\n",
    "\n",
    "        input_ids = (\n",
    "            [self.vocab[\"<sos>\"]] +\n",
    "            text_to_ids_cn(ch, self.vocab) +\n",
    "            [self.vocab[\"<sep>\"]] +\n",
    "            text_to_ids_eng(en, self.vocab) +\n",
    "            [self.vocab[\"<eos>\"]]\n",
    "        )\n",
    "\n",
    "        input_ids = input_ids[:self.max_len]\n",
    "        src = input_ids[:-1]\n",
    "        tgt = input_ids[1:]\n",
    "        # input_ids += [self.vocab[\"<pad>\"]] * (self.max_len - len(input_ids))\n",
    "        src += [self.vocab[\"<pad>\"]]* (self.max_len - len(src))\n",
    "        tgt += [self.vocab[\"<pad>\"]]* (self.max_len - len(tgt))\n",
    "        # return torch.tensor(input_ids[:-1]), torch.tensor(input_ids[1:]) \n",
    "        return torch.tensor(src), torch.tensor(tgt)\n",
    "    \n",
    "\n",
    "vocab = build_vocab(text_pairs)\n",
    "    \n",
    "vocab_size = len(vocab)\n",
    "eval_num = 100 # translate eval_num of inputs\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 49\n",
    "model_max_seq=128\n",
    "max_epoch = 30\n",
    "num_heads = 2\n",
    "\n",
    "\n",
    "dataset = TranslationDataset(text_pairs, vocab, MAX_LEN)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 4.78M\n",
      "running on device cpu\n",
      "iter_dt 0.00ms; iter 0: train loss 9.39832\n",
      "iter_dt 295.72ms; iter 100: train loss 2.29008\n",
      "iter_dt 201.28ms; iter 200: train loss 1.56079\n",
      "iter_dt 246.24ms; iter 300: train loss 1.70178\n",
      "iter_dt 249.06ms; iter 400: train loss 1.96999\n",
      "iter_dt 242.18ms; iter 500: train loss 1.76812\n",
      "iter_dt 245.91ms; iter 600: train loss 1.61267\n",
      "iter_dt 228.36ms; iter 700: train loss 1.66291\n",
      "iter_dt 239.50ms; iter 800: train loss 1.69228\n",
      "iter_dt 252.66ms; iter 900: train loss 1.44959\n",
      "iter_dt 238.86ms; iter 1000: train loss 1.63701\n",
      "iter_dt 252.69ms; iter 1100: train loss 1.57210\n",
      "iter_dt 251.72ms; iter 1200: train loss 1.31319\n",
      "iter_dt 219.13ms; iter 1300: train loss 1.33743\n",
      "iter_dt 241.27ms; iter 1400: train loss 1.34280\n",
      "iter_dt 220.41ms; iter 1500: train loss 1.45900\n",
      "iter_dt 267.05ms; iter 1600: train loss 1.43678\n",
      "iter_dt 249.22ms; iter 1700: train loss 1.30029\n",
      "iter_dt 235.19ms; iter 1800: train loss 1.18036\n",
      "iter_dt 270.38ms; iter 1900: train loss 1.28492\n",
      "iter_dt 260.91ms; iter 2000: train loss 1.32596\n",
      "iter_dt 224.55ms; iter 2100: train loss 1.24914\n",
      "iter_dt 239.68ms; iter 2200: train loss 1.36641\n",
      "iter_dt 230.01ms; iter 2300: train loss 1.45946\n",
      "iter_dt 274.30ms; iter 2400: train loss 1.46592\n",
      "iter_dt 235.08ms; iter 2500: train loss 1.29823\n",
      "iter_dt 237.51ms; iter 2600: train loss 1.08253\n",
      "iter_dt 242.41ms; iter 2700: train loss 1.12421\n",
      "iter_dt 220.01ms; iter 2800: train loss 1.24910\n",
      "iter_dt 244.23ms; iter 2900: train loss 1.13622\n",
      "iter_dt 286.28ms; iter 3000: train loss 1.08984\n",
      "iter_dt 237.28ms; iter 3100: train loss 1.15926\n",
      "iter_dt 244.84ms; iter 3200: train loss 1.20426\n",
      "iter_dt 248.54ms; iter 3300: train loss 1.00941\n",
      "iter_dt 292.94ms; iter 3400: train loss 1.12850\n",
      "iter_dt 269.65ms; iter 3500: train loss 1.03871\n",
      "iter_dt 261.30ms; iter 3600: train loss 1.31935\n",
      "iter_dt 299.43ms; iter 3700: train loss 1.14884\n",
      "iter_dt 257.63ms; iter 3800: train loss 1.21097\n",
      "iter_dt 252.37ms; iter 3900: train loss 1.02565\n",
      "iter_dt 233.68ms; iter 4000: train loss 1.21342\n",
      "iter_dt 220.02ms; iter 4100: train loss 1.06434\n",
      "iter_dt 230.49ms; iter 4200: train loss 1.00313\n",
      "iter_dt 243.50ms; iter 4300: train loss 0.97790\n",
      "iter_dt 288.76ms; iter 4400: train loss 1.04462\n",
      "iter_dt 240.64ms; iter 4500: train loss 1.02109\n",
      "iter_dt 250.88ms; iter 4600: train loss 1.07747\n",
      "iter_dt 267.73ms; iter 4700: train loss 1.12188\n",
      "iter_dt 248.21ms; iter 4800: train loss 1.09836\n",
      "iter_dt 237.95ms; iter 4900: train loss 1.16904\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-mini'\n",
    "model_config.vocab_size = len(vocab) # openai's model vocabulary\n",
    "model_config.block_size = 100  # openai's model block_size (i.e. input context length)\n",
    "model = GPT(model_config)\n",
    "\n",
    "# train config\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4      \n",
    "train_config.batch_size = 32\n",
    "train_config.max_iters = 5000          \n",
    "train_config.num_workers = 0\n",
    "\n",
    "# start training\n",
    "trainer = Trainer(train_config, model, dataset)\n",
    "# trainer.run()\n",
    "\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights save and reload\n",
    "torch.save(model.state_dict(), 'mingpt.pth') # about 26MB\n",
    "model.load_state_dict(torch.load('mingpt.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2w = {v:k for k, v in vocab.items()}\n",
    "def decode(ids, id2w=id2w):\n",
    "    tokens = [id2w.get(t, 'notfound') for t in ids if t !=2]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户输入: 不要再让这种事发生了！\n",
      "正确翻译: Don't let that happen again.\n",
      "模型翻译： dont let this happen again\n",
      "====================\n",
      "用户输入: 你收到我寄給你的包裹了嗎？\n",
      "正确翻译: Did you get the package that I sent you?\n",
      "模型翻译： did you hear my bag\n",
      "====================\n",
      "用户输入: 你在私立高中读书吗？\n",
      "正确翻译: Are you a student of a private high school?\n",
      "模型翻译： are you reading a doctor\n",
      "====================\n",
      "用户输入: 我給了瑪麗一本書。\n",
      "正确翻译: I gave Mary a book.\n",
      "模型翻译： i gave mary a book\n",
      "====================\n",
      "用户输入: 去度假的时候我要租辆车。\n",
      "正确翻译: When I go on vacation, I'll rent a car.\n",
      "模型翻译： i want to get a taxi\n",
      "====================\n",
      "用户输入: 聞起來好香喔。你在煮什麼？\n",
      "正确翻译: This smells great! What are you cooking?\n",
      "模型翻译： the price of you look at the bottle\n",
      "====================\n",
      "用户输入: 湯姆對我不友善。\n",
      "正确翻译: Tom wasn't nice to me.\n",
      "模型翻译： tom is not interested in love\n",
      "====================\n",
      "用户输入: 告訴我你的故事。 我會注意聽。\n",
      "正确翻译: Tell me your story. I am all ears.\n",
      "模型翻译： tell me the story ill hear about it\n",
      "====================\n",
      "用户输入: 请给我看看另一个。\n",
      "正确翻译: Please show me another one.\n",
      "模型翻译： please show me one\n",
      "====================\n",
      "用户输入: 汤姆不知道玛丽是个连环杀手。\n",
      "正确翻译: Tom had no idea that Mary was a serial killer.\n",
      "模型翻译： tom doesnt know mary were a watch\n",
      "====================\n",
      "用户输入: 老師跟他們說，叫他們別在河裡游泳。\n",
      "正确翻译: The teacher told them not to swim in the river.\n",
      "模型翻译： the old man helped them with them are in the river\n",
      "====================\n",
      "用户输入: 我的叔叔有三个孩子。\n",
      "正确翻译: My uncle has three children.\n",
      "模型翻译： my uncle has three children\n",
      "====================\n",
      "用户输入: 他的衬衫被酱汁弄脏了。\n",
      "正确翻译: His shirt was stained with sauce.\n",
      "模型翻译： his shirt was stolen\n",
      "====================\n",
      "用户输入: 我总是有事。\n",
      "正确翻译: I'm always busy.\n",
      "模型翻译： i always have a thing\n",
      "====================\n",
      "用户输入: 我的想法和你的不同。\n",
      "正确翻译: My ideas are different from yours.\n",
      "模型翻译： my idea is different from yours\n",
      "====================\n",
      "用户输入: 我需要一把新的牙刷。\n",
      "正确翻译: I need a new toothbrush.\n",
      "模型翻译： i need a new hair\n",
      "====================\n",
      "用户输入: 我會來你的地方。\n",
      "正确翻译: I'll come to your place.\n",
      "模型翻译： ill come to your place\n",
      "====================\n",
      "用户输入: 她每次看見我都給露出厭惡的眼神。\n",
      "正确翻译: She gives me a nasty look every time she sees me.\n",
      "模型翻译： she saw my eyes see me every time\n",
      "====================\n",
      "用户输入: 你更喜欢哪个，这个还是那个？\n",
      "正确翻译: Which do you like better, this or that?\n",
      "模型翻译： which as you like this is that\n",
      "====================\n",
      "用户输入: 我不喜欢。\n",
      "正确翻译: I don't like this.\n",
      "模型翻译： i dont like\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "model.eval()\n",
    "for i in range(20):\n",
    "    with torch.no_grad():\n",
    "        pair = random.choice(text_pairs)\n",
    "        src = pair[0]\n",
    "        tgt = pair[1]\n",
    "        input_ids = (\n",
    "            [vocab[\"<sos>\"]] +\n",
    "            text_to_ids_cn(src, vocab) +\n",
    "            [vocab[\"<sep>\"]]\n",
    "        )\n",
    "        input_tensor = torch.tensor([input_ids], device=device)\n",
    "        print(f'用户输入: {src}')\n",
    "        print(f'正确翻译: {tgt}')\n",
    "\n",
    "        max_new_token = 20\n",
    "        \n",
    "        gen = model.generate(input_tensor,  max_new_tokens=max_new_token, do_sample=False, top_k=40)\n",
    "        print('模型翻译：',decode(gen[0][-max_new_token:].tolist()))\n",
    "        print('='*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-HW-Py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

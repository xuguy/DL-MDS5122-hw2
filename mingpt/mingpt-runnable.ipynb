{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./minGPT')\n",
    "from mingpt.model import GPT\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "# from mingpt.model import GPT\n",
    " \n",
    "# model_config = GPT.get_default_config()\n",
    "# model_config.vocab_size = len(vocab)  # 词汇表大小\n",
    "# model_config.block_size = 20          # 序列最大长度\n",
    "# model_config.n_layer = 6              # 参考网页6的层数配置\n",
    "# model_config.n_head = 8\n",
    "# model = GPT(model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\GITrepo\\DL-MDS5122-hw2\\mingpt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cpu':\n",
    "    file_path = '../kaggleData/data/eng-cmn.txt'\n",
    "else:\n",
    "    file_path = '/kaggle/input/eng-cmn/eng-cmn.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "\n",
    "        parts = line.strip().split('\\t')\n",
    "        \n",
    "\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "            \n",
    "        en_text = parts[0].strip()\n",
    "        zh_text = parts[1].strip()\n",
    "        \n",
    "\n",
    "        if en_text and zh_text:\n",
    "\n",
    "            text_pairs.append((zh_text, en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "# def build_vocab(text_pairs):\n",
    "#     vocab = defaultdict(lambda: len(vocab))\n",
    "#     special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<sep>\"]\n",
    "#     for token in special_tokens:\n",
    "#         vocab[token]\n",
    "    \n",
    "#     for ch, en in text_pairs:\n",
    "#         for char in (ch + en):\n",
    "#             vocab[char]\n",
    "#     return vocab\n",
    "def build_vocab(text_pairs):\n",
    "    vocab = defaultdict(lambda: len(vocab))\n",
    "    special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<sep>\"]\n",
    "    for token in special_tokens:\n",
    "        vocab[token]\n",
    "    \n",
    "    for ch, en in text_pairs:\n",
    "        ch = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', ch, flags=re.UNICODE)\n",
    "        for char in ch.lower():\n",
    "            vocab[char]\n",
    "        en = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', en, flags=re.UNICODE)\n",
    "        for char in en.lower().split(' '):\n",
    "            vocab[char]\n",
    "    return vocab\n",
    "\n",
    "def text_to_ids_cn(text, vocab, add_special_tokens=False):\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text, flags=re.UNICODE)\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text, flags=re.UNICODE)\n",
    "    text=text.lower()\n",
    "    ids = []\n",
    "    if add_special_tokens:\n",
    "        ids.append(vocab[\"<sos>\"])\n",
    "    for char in text:\n",
    "        ids.append(vocab[char])\n",
    "    # ids += [10, 2476, 2477, 85]\n",
    "    if add_special_tokens:\n",
    "        ids.append(vocab[\"<eos>\"])\n",
    "    return ids\n",
    "\n",
    "def text_to_ids_eng(text, vocab, add_special_tokens=False):\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text, flags=re.UNICODE)\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text, flags=re.UNICODE)\n",
    "    text=text.lower()\n",
    "    ids = []\n",
    "    if add_special_tokens:\n",
    "        ids.append(vocab[\"<sos>\"])\n",
    "    for char in text.split(' '):\n",
    "        ids.append(vocab[char])\n",
    "    \n",
    "    if add_special_tokens:\n",
    "        ids.append(vocab[\"<eos>\"])\n",
    "    return ids\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, vocab, max_len=50):\n",
    "        self.pairs = pairs\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ch, en = self.pairs[idx]\n",
    "\n",
    "        input_ids = (\n",
    "            [self.vocab[\"<sos>\"]] +\n",
    "            text_to_ids_cn(ch, self.vocab) +\n",
    "            [self.vocab[\"<sep>\"]] +\n",
    "            text_to_ids_eng(en, self.vocab) +\n",
    "            [self.vocab[\"<eos>\"]]\n",
    "        )\n",
    "\n",
    "        input_ids = input_ids[:self.max_len]\n",
    "        src = input_ids[:-1]\n",
    "        tgt = input_ids[1:]\n",
    "        # input_ids += [self.vocab[\"<pad>\"]] * (self.max_len - len(input_ids))\n",
    "        src += [self.vocab[\"<pad>\"]]* (self.max_len - len(src))\n",
    "        tgt += [self.vocab[\"<pad>\"]]* (self.max_len - len(tgt))\n",
    "        # return torch.tensor(input_ids[:-1]), torch.tensor(input_ids[1:]) \n",
    "        return torch.tensor(src), torch.tensor(tgt)\n",
    "    \n",
    "\n",
    "vocab = build_vocab(text_pairs)\n",
    "    \n",
    "vocab_size = len(vocab)\n",
    "eval_num = 100 # translate eval_num of inputs\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 49\n",
    "model_max_seq=128\n",
    "max_epoch = 10\n",
    "num_heads = 2\n",
    "\n",
    "\n",
    "dataset = TranslationDataset(text_pairs, vocab, MAX_LEN)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 4.78M\n",
      "running on device cpu\n",
      "iter_dt 0.00ms; iter 0: train loss 9.36291\n",
      "iter_dt 329.57ms; iter 100: train loss 2.09860\n",
      "iter_dt 202.07ms; iter 200: train loss 1.81397\n",
      "iter_dt 248.69ms; iter 300: train loss 1.80720\n",
      "iter_dt 313.52ms; iter 400: train loss 1.68396\n",
      "iter_dt 228.83ms; iter 500: train loss 1.54499\n",
      "iter_dt 219.02ms; iter 600: train loss 1.53388\n",
      "iter_dt 230.85ms; iter 700: train loss 1.50895\n",
      "iter_dt 246.45ms; iter 800: train loss 1.36626\n",
      "iter_dt 248.58ms; iter 900: train loss 1.49999\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "\n",
    "\n",
    "# # 配置GPT模型参数[7](@ref)\n",
    "# config = GPT.get_default_config()\n",
    "# config.model_type = 'gpt-mini'\n",
    "# config.vocab_size = len(dataset.vocab)  # 动态词汇量\n",
    "# config.block_size = dataset.max_length  # 序列最大长度\n",
    "# # config.n_layer = 4                      # 减小层数以适配翻译任务\n",
    "# # config.n_head = 4\n",
    "# model = GPT(config)\n",
    "# from mingpt.model import GPT\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-mini'\n",
    "model_config.vocab_size = len(vocab) # openai's model vocabulary\n",
    "model_config.block_size = 100  # openai's model block_size (i.e. input context length)\n",
    "model = GPT(model_config)\n",
    "# 配置训练参数\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4       # 调高学习率\n",
    "train_config.batch_size = 32\n",
    "train_config.max_iters = 1000          # 减少迭代次数\n",
    "train_config.num_workers = 0\n",
    "\n",
    "# 启动训练\n",
    "trainer = Trainer(train_config, model, dataset)\n",
    "# trainer.run()\n",
    "\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2w = {v:k for k, v in vocab.items()}\n",
    "def decode(ids, id2w=id2w):\n",
    "    tokens = [id2w.get(t, 'notfound') for t in ids if t !=2]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 1859, 6, 10, 2358]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_ids_cn('我测你的马', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户输入: 帮我买一橘子\n",
      "正确翻译: \n",
      "模型翻译： please tell me the door\n",
      "====================\n",
      "用户输入: 帮我买一橘子\n",
      "正确翻译: \n",
      "模型翻译： please tell me the door\n",
      "====================\n",
      "用户输入: 帮我买一橘子\n",
      "正确翻译: \n",
      "模型翻译： please tell me the door\n",
      "====================\n",
      "用户输入: 帮我买一橘子\n",
      "正确翻译: \n",
      "模型翻译： please tell me the door\n",
      "====================\n",
      "用户输入: 帮我买一橘子\n",
      "正确翻译: \n",
      "模型翻译： please tell me the door\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    src = '帮我买一橘子' # text_pairs[i][1]\n",
    "    tgt = ''\n",
    "    input_ids = (\n",
    "        [vocab[\"<sos>\"]] +\n",
    "        text_to_ids_cn(src, vocab) +\n",
    "        [vocab[\"<sep>\"]]\n",
    "    )\n",
    "    input_tensor = torch.tensor([input_ids], device=device)\n",
    "    print(f'用户输入: {src}')\n",
    "    print(f'正确翻译: {tgt}')\n",
    "\n",
    "    max_new_token = 20\n",
    "    model.eval()\n",
    "    gen = model.generate(input_tensor,  max_new_tokens=max_new_token, do_sample=False, top_k=40)\n",
    "    print('模型翻译：',decode(gen[0][-max_new_token:].tolist()))\n",
    "    print('='*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-HW-Py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

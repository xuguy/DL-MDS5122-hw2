{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./minGPT')\n",
    "from mingpt.model import GPT\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\GITrepo\\DL-MDS5122-hw2\\mingpt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cpu':\n",
    "    file_path = '../kaggleData/data/eng-cmn.txt'\n",
    "else:\n",
    "    file_path = '/kaggle/input/eng-cmn/eng-cmn.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        \n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "            \n",
    "        en_text = parts[0].strip()\n",
    "        zh_text = parts[1].strip()\n",
    "    \n",
    "        if en_text and zh_text:\n",
    "            text_pairs.append((zh_text, en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(text_pairs):\n",
    "    vocab = defaultdict(lambda: len(vocab))\n",
    "    special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<sep>\"]\n",
    "    for token in special_tokens:\n",
    "        vocab[token]\n",
    "    \n",
    "    for ch, en in text_pairs:\n",
    "        ch = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', ch, flags=re.UNICODE)\n",
    "        for char in ch.lower():\n",
    "            vocab[char]\n",
    "        en = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', en, flags=re.UNICODE)\n",
    "        for char in en.lower().split(' '):\n",
    "            vocab[char]\n",
    "    return vocab\n",
    "\n",
    "def text_to_ids_cn(text, vocab, add_special_tokens=False):\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text, flags=re.UNICODE)\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text, flags=re.UNICODE)\n",
    "    text=text.lower()\n",
    "    ids = []\n",
    "    if add_special_tokens:\n",
    "        ids.append(vocab[\"<sos>\"])\n",
    "    for char in text:\n",
    "        ids.append(vocab[char])\n",
    "    # ids += [10, 2476, 2477, 85]\n",
    "    if add_special_tokens:\n",
    "        ids.append(vocab[\"<eos>\"])\n",
    "    return ids\n",
    "\n",
    "def text_to_ids_eng(text, vocab, add_special_tokens=False):\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text, flags=re.UNICODE)\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text, flags=re.UNICODE)\n",
    "    text=text.lower()\n",
    "    ids = []\n",
    "    if add_special_tokens:\n",
    "        ids.append(vocab[\"<sos>\"])\n",
    "    for char in text.split(' '):\n",
    "        ids.append(vocab[char])\n",
    "    \n",
    "    if add_special_tokens:\n",
    "        ids.append(vocab[\"<eos>\"])\n",
    "    return ids\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, vocab, max_len=50):\n",
    "        self.pairs = pairs\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ch, en = self.pairs[idx]\n",
    "\n",
    "        input_ids = (\n",
    "            [self.vocab[\"<sos>\"]] +\n",
    "            text_to_ids_cn(ch, self.vocab) +\n",
    "            [self.vocab[\"<sep>\"]] +\n",
    "            text_to_ids_eng(en, self.vocab) +\n",
    "            [self.vocab[\"<eos>\"]]\n",
    "        )\n",
    "\n",
    "        input_ids = input_ids[:self.max_len]\n",
    "        src = input_ids[:-1]\n",
    "        tgt = input_ids[1:]\n",
    "        # input_ids += [self.vocab[\"<pad>\"]] * (self.max_len - len(input_ids))\n",
    "        src += [self.vocab[\"<pad>\"]]* (self.max_len - len(src))\n",
    "        tgt += [self.vocab[\"<pad>\"]]* (self.max_len - len(tgt))\n",
    "        # return torch.tensor(input_ids[:-1]), torch.tensor(input_ids[1:]) \n",
    "        return torch.tensor(src), torch.tensor(tgt)\n",
    "    \n",
    "\n",
    "# build dataset and dataloader\n",
    "vocab = build_vocab(text_pairs)\n",
    "vocab_size = len(vocab)\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 45\n",
    "\n",
    "dataset = TranslationDataset(text_pairs, vocab, MAX_LEN)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 4.78M\n",
      "running on device cpu\n",
      "iter_dt 0.00ms; iter 0: train loss 9.30095\n",
      "iter_dt 179.11ms; iter 100: train loss 2.17026\n",
      "iter_dt 183.84ms; iter 200: train loss 1.90296\n",
      "iter_dt 178.93ms; iter 300: train loss 1.97812\n",
      "iter_dt 179.58ms; iter 400: train loss 1.91965\n",
      "iter_dt 234.74ms; iter 500: train loss 1.86092\n",
      "iter_dt 191.83ms; iter 600: train loss 1.72459\n",
      "iter_dt 222.67ms; iter 700: train loss 1.92066\n",
      "iter_dt 186.77ms; iter 800: train loss 1.61618\n",
      "iter_dt 232.92ms; iter 900: train loss 1.81775\n",
      "iter_dt 218.06ms; iter 1000: train loss 1.48267\n",
      "iter_dt 179.22ms; iter 1100: train loss 1.65501\n",
      "iter_dt 198.70ms; iter 1200: train loss 1.61101\n",
      "iter_dt 183.27ms; iter 1300: train loss 1.62081\n",
      "iter_dt 179.49ms; iter 1400: train loss 1.48914\n",
      "iter_dt 211.45ms; iter 1500: train loss 1.42379\n",
      "iter_dt 182.45ms; iter 1600: train loss 1.34717\n",
      "iter_dt 211.42ms; iter 1700: train loss 1.37301\n",
      "iter_dt 205.69ms; iter 1800: train loss 1.45805\n",
      "iter_dt 173.50ms; iter 1900: train loss 1.57338\n",
      "iter_dt 189.44ms; iter 2000: train loss 1.42699\n",
      "iter_dt 219.70ms; iter 2100: train loss 1.69610\n",
      "iter_dt 189.23ms; iter 2200: train loss 1.32704\n",
      "iter_dt 216.27ms; iter 2300: train loss 1.16289\n",
      "iter_dt 181.03ms; iter 2400: train loss 1.31586\n",
      "iter_dt 186.73ms; iter 2500: train loss 1.35076\n",
      "iter_dt 180.78ms; iter 2600: train loss 1.39492\n",
      "iter_dt 195.14ms; iter 2700: train loss 1.30823\n",
      "iter_dt 195.47ms; iter 2800: train loss 1.46467\n",
      "iter_dt 187.80ms; iter 2900: train loss 1.57624\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-mini' # use 'gpt-mini' for better translation performance, weights ~ 26MB\n",
    "model_config.vocab_size = len(vocab) # model vocabulary\n",
    "model_config.block_size = 128  # model block_size (i.e. input context length)\n",
    "model = GPT(model_config)\n",
    "\n",
    "# train config\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4      \n",
    "train_config.batch_size = 32\n",
    "train_config.max_iters = 3000          \n",
    "train_config.num_workers = 0\n",
    "\n",
    "# start training\n",
    "trainer = Trainer(train_config, model, dataset)\n",
    "# trainer.run()\n",
    "\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run() # gpt-mini: 10min for 3000 iters\n",
    "torch.save(model.state_dict(), 'mingpt.pth') # about 26MB for gpt-min, 4.5MB for gpt-nano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights reload\n",
    "model.load_state_dict(torch.load('mingpt.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2w = {v:k for k, v in vocab.items()}\n",
    "def decode(ids, id2w=id2w):\n",
    "    tokens = [id2w.get(t, 'notfound') for t in ids if t !=2]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户输入: 你可以选择任何你想要的。\n",
      "正确翻译: You may choose whichever you want.\n",
      "模型翻译： you can think you want to be right\n",
      "====================\n",
      "用户输入: 你绝对肯定吗？\n",
      "正确翻译: Are you absolutely sure?\n",
      "模型翻译： are you sure about it\n",
      "====================\n",
      "用户输入: 他們從我的果園偷了蘋果。\n",
      "正确翻译: They stole apples from my orchard.\n",
      "模型翻译： they went out of the apple apple\n",
      "====================\n",
      "用户输入: 汤姆对局势一无所知。\n",
      "正确翻译: Tom knows nothing about the situation.\n",
      "模型翻译： tom doesnt know that mary is a thing\n",
      "====================\n",
      "用户输入: 我们的车走了。\n",
      "正确翻译: There goes our bus.\n",
      "模型翻译： our car went\n",
      "====================\n",
      "用户输入: 他們一整年都必須工作。\n",
      "正确翻译: They had to work all year round.\n",
      "模型翻译： they have to work all day\n",
      "====================\n",
      "用户输入: 赶快回家。\n",
      "正确翻译: Go home quickly.\n",
      "模型翻译： get back home\n",
      "====================\n",
      "用户输入: 這個柳橙太酸了。\n",
      "正确翻译: This orange is too sour.\n",
      "模型翻译： the apple juice\n",
      "====================\n",
      "用户输入: 请说得更清楚些。\n",
      "正确翻译: Please speak more clearly.\n",
      "模型翻译： please say more more more\n",
      "====================\n",
      "用户输入: 我喜欢你走路的方式。\n",
      "正确翻译: I like the way you walk.\n",
      "模型翻译： i like you to go there\n",
      "====================\n",
      "用户输入: 我把他的书还给了他。\n",
      "正确翻译: I returned his book to him.\n",
      "模型翻译： i gave him my book\n",
      "====================\n",
      "用户输入: 他娶了一位空姐。\n",
      "正确翻译: He married a stewardess.\n",
      "模型翻译： he gave a favorite country\n",
      "====================\n",
      "用户输入: 如果值得一做，就值得做好。\n",
      "正确翻译: If it is worth doing at all, it is worth doing well.\n",
      "模型翻译： if you are too happy to do that\n",
      "====================\n",
      "用户输入: 他很快就会回来。\n",
      "正确翻译: He will be back in a second.\n",
      "模型翻译： he will come back back back\n",
      "====================\n",
      "用户输入: 我一星期沒帶食物去。\n",
      "正确翻译: I went without food for a week.\n",
      "模型翻译： i didnt take a food to go to work last night\n",
      "====================\n",
      "用户输入: 他舉起了他的手。\n",
      "正确翻译: He raised his hands.\n",
      "模型翻译： he began to his hands\n",
      "====================\n",
      "用户输入: 这套房子设施便利齐全。\n",
      "正确翻译: The house has all the conveniences.\n",
      "模型翻译： this house is as a house as you are\n",
      "====================\n",
      "用户输入: 我知道湯姆在看我。\n",
      "正确翻译: I know Tom is watching me.\n",
      "模型翻译： i know tom is looking at me\n",
      "====================\n",
      "用户输入: 我希望你很快就會好。\n",
      "正确翻译: I hope you'll get well soon.\n",
      "模型翻译： i hope you can do it soon\n",
      "====================\n",
      "用户输入: 讓我們放學後打網球。\n",
      "正确翻译: Let's play tennis after school.\n",
      "模型翻译： lets play tennis after school\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "max_new_token = 20 # 20 is enough for eval\n",
    "model.eval()\n",
    "for i in range(20):\n",
    "    with torch.no_grad():\n",
    "        pair = random.choice(text_pairs)\n",
    "        src = pair[0]\n",
    "        tgt = pair[1]\n",
    "        input_ids = (\n",
    "            [vocab[\"<sos>\"]] +\n",
    "            text_to_ids_cn(src, vocab) +\n",
    "            [vocab[\"<sep>\"]]\n",
    "        )\n",
    "        input_tensor = torch.tensor([input_ids], device=device)\n",
    "        print(f'用户输入: {src}')\n",
    "        print(f'正确翻译: {tgt}')\n",
    "\n",
    "        gen = model.generate(input_tensor,  max_new_tokens=max_new_token, do_sample=False, top_k=40)\n",
    "        print('模型翻译：',decode(gen[0][-max_new_token:].tolist()))\n",
    "        print('='*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-HW-Py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./minGPT')\n",
    "from mingpt.model import GPT\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "# from mingpt.model import GPT\n",
    " \n",
    "# model_config = GPT.get_default_config()\n",
    "# model_config.vocab_size = len(vocab)  # 词汇表大小\n",
    "# model_config.block_size = 20          # 序列最大长度\n",
    "# model_config.n_layer = 6              # 参考网页6的层数配置\n",
    "# model_config.n_head = 8\n",
    "# model = GPT(model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\GITrepo\\DL-MDS5122-hw2\\mingpt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cpu':\n",
    "    file_path = '../kaggleData/data/eng-cmn.txt'\n",
    "else:\n",
    "    file_path = '/kaggle/input/eng-cmn/eng-cmn.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "\n",
    "        parts = line.strip().split('\\t')\n",
    "        \n",
    "\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "            \n",
    "        en_text = parts[0].strip()\n",
    "        zh_text = parts[1].strip()\n",
    "        \n",
    "\n",
    "        if en_text and zh_text:\n",
    "\n",
    "            text_pairs.append((zh_text, en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "# def build_vocab(text_pairs):\n",
    "#     vocab = defaultdict(lambda: len(vocab))\n",
    "#     special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<sep>\"]\n",
    "#     for token in special_tokens:\n",
    "#         vocab[token]\n",
    "    \n",
    "#     for ch, en in text_pairs:\n",
    "#         for char in (ch + en):\n",
    "#             vocab[char]\n",
    "#     return vocab\n",
    "def build_vocab(text_pairs):\n",
    "    vocab = defaultdict(lambda: len(vocab))\n",
    "    special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<sep>\"]\n",
    "    for token in special_tokens:\n",
    "        vocab[token]\n",
    "    \n",
    "    for ch, en in text_pairs:\n",
    "        ch = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', ch, flags=re.UNICODE)\n",
    "        for char in ch.lower():\n",
    "            vocab[char]\n",
    "        en = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', en, flags=re.UNICODE)\n",
    "        for char in en.lower().split(' '):\n",
    "            vocab[char]\n",
    "    return vocab\n",
    "\n",
    "def text_to_ids_cn(text, vocab, add_special_tokens=False):\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text, flags=re.UNICODE)\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text, flags=re.UNICODE)\n",
    "    text=text.lower()\n",
    "    ids = []\n",
    "    if add_special_tokens:\n",
    "        ids.append(vocab[\"<sos>\"])\n",
    "    for char in text:\n",
    "        ids.append(vocab[char])\n",
    "    # ids += [10, 2476, 2477, 85]\n",
    "    if add_special_tokens:\n",
    "        ids.append(vocab[\"<eos>\"])\n",
    "    return ids\n",
    "\n",
    "def text_to_ids_eng(text, vocab, add_special_tokens=False):\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text, flags=re.UNICODE)\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text, flags=re.UNICODE)\n",
    "    text=text.lower()\n",
    "    ids = []\n",
    "    if add_special_tokens:\n",
    "        ids.append(vocab[\"<sos>\"])\n",
    "    for char in text.split(' '):\n",
    "        ids.append(vocab[char])\n",
    "    \n",
    "    if add_special_tokens:\n",
    "        ids.append(vocab[\"<eos>\"])\n",
    "    return ids\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, vocab, max_len=50):\n",
    "        self.pairs = pairs\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ch, en = self.pairs[idx]\n",
    "\n",
    "        input_ids = (\n",
    "            [self.vocab[\"<sos>\"]] +\n",
    "            text_to_ids_cn(ch, self.vocab) +\n",
    "            [self.vocab[\"<sep>\"]] +\n",
    "            text_to_ids_eng(en, self.vocab) +\n",
    "            [self.vocab[\"<eos>\"]]\n",
    "        )\n",
    "\n",
    "        input_ids = input_ids[:self.max_len]\n",
    "        src = input_ids[:-1]\n",
    "        tgt = input_ids[1:]\n",
    "        # input_ids += [self.vocab[\"<pad>\"]] * (self.max_len - len(input_ids))\n",
    "        src += [self.vocab[\"<pad>\"]]* (self.max_len - len(src))\n",
    "        tgt += [self.vocab[\"<pad>\"]]* (self.max_len - len(tgt))\n",
    "        # return torch.tensor(input_ids[:-1]), torch.tensor(input_ids[1:]) \n",
    "        return torch.tensor(src), torch.tensor(tgt)\n",
    "    \n",
    "\n",
    "vocab = build_vocab(text_pairs)\n",
    "    \n",
    "vocab_size = len(vocab)\n",
    "eval_num = 100 # translate eval_num of inputs\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 49\n",
    "model_max_seq=128\n",
    "max_epoch = 30\n",
    "num_heads = 2\n",
    "\n",
    "\n",
    "dataset = TranslationDataset(text_pairs, vocab, MAX_LEN)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 4.78M\n",
      "running on device cpu\n",
      "iter_dt 0.00ms; iter 0: train loss 9.39832\n",
      "iter_dt 295.72ms; iter 100: train loss 2.29008\n",
      "iter_dt 201.28ms; iter 200: train loss 1.56079\n",
      "iter_dt 246.24ms; iter 300: train loss 1.70178\n",
      "iter_dt 249.06ms; iter 400: train loss 1.96999\n",
      "iter_dt 242.18ms; iter 500: train loss 1.76812\n",
      "iter_dt 245.91ms; iter 600: train loss 1.61267\n",
      "iter_dt 228.36ms; iter 700: train loss 1.66291\n",
      "iter_dt 239.50ms; iter 800: train loss 1.69228\n",
      "iter_dt 252.66ms; iter 900: train loss 1.44959\n",
      "iter_dt 238.86ms; iter 1000: train loss 1.63701\n",
      "iter_dt 252.69ms; iter 1100: train loss 1.57210\n",
      "iter_dt 251.72ms; iter 1200: train loss 1.31319\n",
      "iter_dt 219.13ms; iter 1300: train loss 1.33743\n",
      "iter_dt 241.27ms; iter 1400: train loss 1.34280\n",
      "iter_dt 220.41ms; iter 1500: train loss 1.45900\n",
      "iter_dt 267.05ms; iter 1600: train loss 1.43678\n",
      "iter_dt 249.22ms; iter 1700: train loss 1.30029\n",
      "iter_dt 235.19ms; iter 1800: train loss 1.18036\n",
      "iter_dt 270.38ms; iter 1900: train loss 1.28492\n",
      "iter_dt 260.91ms; iter 2000: train loss 1.32596\n",
      "iter_dt 224.55ms; iter 2100: train loss 1.24914\n",
      "iter_dt 239.68ms; iter 2200: train loss 1.36641\n",
      "iter_dt 230.01ms; iter 2300: train loss 1.45946\n",
      "iter_dt 274.30ms; iter 2400: train loss 1.46592\n",
      "iter_dt 235.08ms; iter 2500: train loss 1.29823\n",
      "iter_dt 237.51ms; iter 2600: train loss 1.08253\n",
      "iter_dt 242.41ms; iter 2700: train loss 1.12421\n",
      "iter_dt 220.01ms; iter 2800: train loss 1.24910\n",
      "iter_dt 244.23ms; iter 2900: train loss 1.13622\n",
      "iter_dt 286.28ms; iter 3000: train loss 1.08984\n",
      "iter_dt 237.28ms; iter 3100: train loss 1.15926\n",
      "iter_dt 244.84ms; iter 3200: train loss 1.20426\n",
      "iter_dt 248.54ms; iter 3300: train loss 1.00941\n",
      "iter_dt 292.94ms; iter 3400: train loss 1.12850\n",
      "iter_dt 269.65ms; iter 3500: train loss 1.03871\n",
      "iter_dt 261.30ms; iter 3600: train loss 1.31935\n",
      "iter_dt 299.43ms; iter 3700: train loss 1.14884\n",
      "iter_dt 257.63ms; iter 3800: train loss 1.21097\n",
      "iter_dt 252.37ms; iter 3900: train loss 1.02565\n",
      "iter_dt 233.68ms; iter 4000: train loss 1.21342\n",
      "iter_dt 220.02ms; iter 4100: train loss 1.06434\n",
      "iter_dt 230.49ms; iter 4200: train loss 1.00313\n",
      "iter_dt 243.50ms; iter 4300: train loss 0.97790\n",
      "iter_dt 288.76ms; iter 4400: train loss 1.04462\n",
      "iter_dt 240.64ms; iter 4500: train loss 1.02109\n",
      "iter_dt 250.88ms; iter 4600: train loss 1.07747\n",
      "iter_dt 267.73ms; iter 4700: train loss 1.12188\n",
      "iter_dt 248.21ms; iter 4800: train loss 1.09836\n",
      "iter_dt 237.95ms; iter 4900: train loss 1.16904\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "\n",
    "\n",
    "# # 配置GPT模型参数[7](@ref)\n",
    "# config = GPT.get_default_config()\n",
    "# config.model_type = 'gpt-mini'\n",
    "# config.vocab_size = len(dataset.vocab)  # 动态词汇量\n",
    "# config.block_size = dataset.max_length  # 序列最大长度\n",
    "# # config.n_layer = 4                      # 减小层数以适配翻译任务\n",
    "# # config.n_head = 4\n",
    "# model = GPT(config)\n",
    "# from mingpt.model import GPT\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-mini'\n",
    "model_config.vocab_size = len(vocab) # openai's model vocabulary\n",
    "model_config.block_size = 100  # openai's model block_size (i.e. input context length)\n",
    "model = GPT(model_config)\n",
    "# 配置训练参数\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4      \n",
    "train_config.batch_size = 32\n",
    "train_config.max_iters = 5000          \n",
    "train_config.num_workers = 0\n",
    "\n",
    "# 启动训练\n",
    "trainer = Trainer(train_config, model, dataset)\n",
    "# trainer.run()\n",
    "\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2w = {v:k for k, v in vocab.items()}\n",
    "def decode(ids, id2w=id2w):\n",
    "    tokens = [id2w.get(t, 'notfound') for t in ids if t !=2]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('嗨。', 'Hi.')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户输入: 我每天都给他打电话。\n",
      "正确翻译: I phone him every day.\n",
      "模型翻译： i dont go this book\n",
      "====================\n",
      "用户输入: 你们自己吃蛋糕。\n",
      "正确翻译: Please help yourself to the cake.\n",
      "模型翻译： your favorite exhausted\n",
      "====================\n",
      "用户输入: 法语不是汤姆的母语。\n",
      "正确翻译: French isn't Tom's native language.\n",
      "模型翻译： tom is a good brother\n",
      "====================\n",
      "用户输入: 我的房子又舊又難看。\n",
      "正确翻译: My house is old and ugly.\n",
      "模型翻译： my father is a lot of the same\n",
      "====================\n",
      "用户输入: 請思考一下明天。\n",
      "正确翻译: Think about tomorrow.\n",
      "模型翻译： please be a good week\n",
      "====================\n",
      "用户输入: 因為以前見過他, 所以我立刻就認出他來。\n",
      "正确翻译: Having met him before, I recognized him at once.\n",
      "模型翻译： he was not to me that he was going to be\n",
      "====================\n",
      "用户输入: 這一個還活著。\n",
      "正确翻译: This one's still alive.\n",
      "模型翻译： this is a good\n",
      "====================\n",
      "用户输入: 他们就快从香港抵达了。\n",
      "正确翻译: They are arriving here soon from Hong Kong.\n",
      "模型翻译： they have to the train\n",
      "====================\n",
      "用户输入: 她會唱歌而且舞跳得很美。\n",
      "正确翻译: She can sing and dance beautifully.\n",
      "模型翻译： she is not to be a good\n",
      "====================\n",
      "用户输入: 汤姆睡着了。\n",
      "正确翻译: Tom fell asleep.\n",
      "模型翻译： tom is a good\n",
      "====================\n",
      "用户输入: 你們家在哪？\n",
      "正确翻译: Where is your house?\n",
      "模型翻译： where are you\n",
      "====================\n",
      "用户输入: 我有跟你同樣的麻煩。\n",
      "正确翻译: I have the same trouble as you have.\n",
      "模型翻译： i have you to the same\n",
      "====================\n",
      "用户输入: 我想改善我的英语发音。\n",
      "正确翻译: I would like to improve my English pronunciation.\n",
      "模型翻译： i want to my brother to my name\n",
      "====================\n",
      "用户输入: 我们不这样认为。\n",
      "正确翻译: We don't think so.\n",
      "模型翻译： we dont know this\n",
      "====================\n",
      "用户输入: 她強迫他吃菠菜。\n",
      "正确翻译: She forced him to eat spinach.\n",
      "模型翻译： she was a good than he was a good\n",
      "====================\n",
      "用户输入: 你喜歡下雨嗎？\n",
      "正确翻译: Do you like rain?\n",
      "模型翻译： do you like\n",
      "====================\n",
      "用户输入: 我不可以喝牛奶。\n",
      "正确翻译: I can't drink milk.\n",
      "模型翻译： i dont like a good week\n",
      "====================\n",
      "用户输入: 我是芬兰人，但我也说瑞典语。\n",
      "正确翻译: I am Finnish, but I speak also Swedish.\n",
      "模型翻译： i know that i was my brother\n",
      "====================\n",
      "用户输入: 那个我以为诚实的男孩欺骗了我。\n",
      "正确翻译: The boy I thought was honest deceived me.\n",
      "模型翻译： i was my father is my house\n",
      "====================\n",
      "用户输入: 你确定他们可以干这个吗？\n",
      "正确翻译: Are you sure they can do this?\n",
      "模型翻译： do you know that he was\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(20):\n",
    "    pair = random.choice(text_pairs)\n",
    "    src = pair[0]\n",
    "    tgt = pair[1]\n",
    "    input_ids = (\n",
    "        [vocab[\"<sos>\"]] +\n",
    "        text_to_ids_cn(src, vocab) +\n",
    "        [vocab[\"<sep>\"]]\n",
    "    )\n",
    "    input_tensor = torch.tensor([input_ids], device=device)\n",
    "    print(f'用户输入: {src}')\n",
    "    print(f'正确翻译: {tgt}')\n",
    "\n",
    "    max_new_token = 20\n",
    "    model.eval()\n",
    "    gen = model.generate(input_tensor,  max_new_tokens=max_new_token, do_sample=False, top_k=40)\n",
    "    print('模型翻译：',decode(gen[0][-max_new_token:].tolist()))\n",
    "    print('='*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-HW-Py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

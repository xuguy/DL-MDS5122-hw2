---
Trained model weights are available at github repo:
---

File description:

<u>Do not alter files structure</u>

1. `dl-hw2_seq2seq_kaggle.ipynb` and `dl-hw2_gpt_kaggle.ipynb` are the codes of Task A, this 2 notebook are ready to run in both local machine and cloud-end GPU, we also prepare the trained weights for both `seq2seq` model and `GPT` model so that you don't need to train it your self.

2. `dl-hw2-partb-loss-raw.ipynb` also runnable (ready-to-run, just click `run all` in your device, atleast 16GB CUDA RAM) in Kaggle, but please do not try to comprehend the code yourself, you can send me an email at `224040074@link.cuhk.edu.cn` to explain the code to you via an online meeting to save your time.

3. `dl-hw2-report.<>` report source code (markdown) and html

---
<u>you can safely ignore the following files</u>

---

4. the 2 `kernel-metadata_<>.json` is the config file of sending codes to run in `Kaggle`, you can ignore it, or modify them to run in your `Kaggle` account.

5. `misc` store the images and data used to write the report `dl-hw2-report.md`, as well as model weights (weights only available at github repo to save space to upload to BlackBoard Learn)

6. `mingpt` store the code and a ready-to-run `.ipynb` GPT model for translating (Task A), <u>**you can ignore this file</u> because this is just for experiment and benchmark to test how well a `GPT` model can do in translation task comparing with the one we build from scratch at `dl-hw2_gpt_kaggle.ipynb`

7. `trash` ignore, store all old version of codes or wasted files.


{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %%\n!pip install -q h5py typing-extensions wheel\n!pip install -q -U bitsandbytes\n# !pip install -q -U git+https://github.com/huggingface/transformers.git\n# !pip install -q -U git+https://github.com/huggingface/peft.git\n# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n# !pip install -q datasets\n\n# %%\n!nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Load Pre-trained model and tokenizer\n\n# %%\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nmodel_id = \"Qwen/Qwen2.5-3B-Instruct\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True, # Activate nested quantization for 4-bit base models (double quantization)\n    bnb_4bit_quant_type=\"nf4\", # Quantization type (fp4 or nf4), According to QLoRA paper, for training 4-bit base models (e.g. using LoRA adapters) one should use\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n\n# %% [markdown]\n# ## Preprocess the quantized model for training\n\n# %%\nfrom peft import prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n\n# %%\nfrom peft import LoraConfig, get_peft_model\n\n# You can try differnt parameter-effient strategy for model trianing, for more info, please check https://github.com/huggingface/peft\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=8,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, config)\nprint(\"cell finished\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [markdown]\n# ## Chat Template Usage\n\n# %%\nfrom jinja2 import Template\ntemplate = Template(tokenizer.chat_template)\nmessage = \"Please introduce yourself\"\nprint(f\"message:\\n{message}\\n\")\nmessage_send_to_model=template.render(messages=[{\"role\": \"user\", \"content\": message}],bos_token=tokenizer.bos_token,add_generation_prompt=True)\nprint(f\"message_send_to_model:\\n{message_send_to_model}\")\n\n# %%\ntemplate = Template(tokenizer.chat_template)\n\n@torch.no_grad()\ndef generate(prompt):\n    modelInput=template.render(messages=[{\"role\": \"user\", \"content\": prompt}],bos_token= tokenizer.bos_token,add_generation_prompt=True)\n    print(\"-\"*80)\n    print(f\"model_input_string:\\n{modelInput}\")\n    input_ids = tokenizer.encode(modelInput, add_special_tokens=False, return_tensors='pt').to(\"cuda:0\")\n    outputs = model.generate(input_ids, do_sample=False)\n    model_return_string = tokenizer.decode(*outputs, skip_special_tokens=False)\n    print(\"-\"*80)\n    print(f\"model_return_string:\\n{model_return_string}\")\n    generated_ids = outputs[:, input_ids.shape[1]:]\n    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n    return generated_text\n\nquery = \"Please introduce yourself\"\nprint(\"-\"*80)\nprint(f\"query:\\n{query}\")\nresponse = generate(query)\nprint(\"-\"*80)\nprint(f\"response:\\n{response}\")\n\n# %% [markdown]\n# ## Data Preparation\n\n# %% [markdown]\n# Let's load a common dataset, english quotes, to fine tune our model on famous quotes.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# original huatuo dataset\n# # %%\n# from datasets import load_dataset\n\n# data = load_dataset(\"Abirate/english_quotes\")\n\n# dataset = load_dataset(\"FreedomIntelligence/Huatuo26M-Lite\")\n# dataset = dataset['train'].map(lambda sample: {\"conversations\": [{\"from\": \"human\", \"value\": sample['question']}, {\"from\": \"gpt\", \"value\": sample['answer']}]}, batched=False)\n\n# dataset[3]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load 法律数据集\nfrom datasets import load_dataset, Features, Value, Sequence, concatenate_datasets\n\n# 定义column转换方法\ndef convert_to_conversation(sample):\n    return {\n        \"conversations\": [\n            {\"from\": \"human\", \"value\":  sample['input']},\n            {\"from\": \"gpt\", \"value\": sample['output']}\n        ]\n    }\n\ndataset = load_dataset(\"ShengbinYue/DISC-Law-SFT\", data_files=['DISC-Law-SFT-Pair-QA-released.jsonl', 'DISC-Law-SFT-Pair.jsonl'])\n\n# 应用转换\ndataset = dataset['train'].map(\n    convert_to_conversation,\n    remove_columns=dataset['train'].column_names,  # 移除原始列\n    batched=False\n)\n\nfeatures = Features({\n    'id': Value('string'),\n    'reference': Sequence(Value('string')),\n    'input': Value('string'),\n    'output': Value('string')\n})\n\ndataset2 = load_dataset(\"ShengbinYue/DISC-Law-SFT\", features = features,data_files=['DISC-Law-SFT-Triplet-QA-released.jsonl', 'DISC-Law-SFT-Triplet-released.jsonl'])\n\ndataset2 = dataset2['train'].map(\n    convert_to_conversation,\n    remove_columns=dataset2['train'].column_names,  # 移除原始列\n    batched=False\n)\n\nmerged_dataset = concatenate_datasets([dataset, dataset2])\nmerged_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n# model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n# tokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\nfrom torch.utils.data import random_split\ntrain_dataset_size, val_dataset_size = 20, 8\ntrain_dataset, val_dataset, _ = random_split(dataset, [train_dataset_size, val_dataset_size, len(dataset)-train_dataset_size-val_dataset_size])\n# train_dataset, val_dataset, _ = random_split(dataset2, [0.5,0.1,0.4])\n# print(train_dataset[0]['conversations'])\n\n# %% [markdown]\n# ### Customized Dataset\n# Create a specialized dataset class named \"InstructionDataset\" designed to handle our custom dataset.\n\n# %%\nimport transformers\nfrom typing import Dict, Sequence, List\nfrom torch.utils.data import Dataset\nfrom dataclasses import dataclass\n\ndef preprocess(\n    sources,\n    tokenizer: transformers.PreTrainedTokenizer,\n) -> Dict:\n    template = Template(tokenizer.chat_template)\n    max_seq_len = tokenizer.model_max_length\n    messages = []\n    for i, source in enumerate(sources):\n        if source[0][\"from\"] != \"human\":\n            # Skip the first one if it is not from human\n            source = source[1:]\n\n        for j in range(0, len(source), 2):\n            if j+1 >= len(source): continue\n            q = source[j][\"value\"]\n            a = source[j+1][\"value\"]\n            assert q is not None and a is not None, f'q:{q} a:{a}'\n            input =  template.render(messages=[{\"role\": \"user\", \"content\": q},{\"role\": \"assistant\", \"content\": a}],bos_token=tokenizer.bos_token,add_generation_prompt=False)\n            input_ids = tokenizer.encode(input, add_special_tokens= False)\n\n            query = template.render(messages=[{\"role\": \"user\", \"content\": q}],bos_token=tokenizer.bos_token,add_generation_prompt=True)\n            query_ids = tokenizer.encode(query, add_special_tokens= False)\n\n            labels = [-100]*len(query_ids) + input_ids[len(query_ids):]\n            assert len(labels) == len(input_ids)\n            if len(input_ids) == 0: continue\n            messages.append({\"input_ids\": input_ids[-max_seq_len:], \"labels\": labels[-max_seq_len:]})\n\n    input_ids = [item[\"input_ids\"] for item in messages]\n    labels = [item[\"labels\"] for item in messages]\n\n    max_len = max(len(x) for x in input_ids)\n\n    max_len = min(max_len, max_seq_len)\n    input_ids = [ item[:max_len] + [tokenizer.eos_token_id]*(max_len-len(item)) for item in input_ids]\n    labels = [ item[:max_len] + [-100]*(max_len-len(item)) for item in labels]\n\n    input_ids = torch.LongTensor(input_ids)\n    labels = torch.LongTensor(labels)\n    return {\n        \"input_ids\": input_ids,\n        \"labels\": labels\n    }\n\n\nclass InstructDataset(Dataset):\n    def __init__(self, data: Sequence, tokenizer: transformers.PreTrainedTokenizer) -> None:\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index) -> Dict[str, torch.Tensor]:\n        sources = self.data[index]\n        if isinstance(index, int):\n            sources = [sources]\n        data_dict = preprocess([e['conversations'] for e in sources], self.tokenizer)\n        if isinstance(index, int):\n            data_dict = dict(input_ids=data_dict[\"input_ids\"][0], labels=data_dict[\"labels\"][0])\n        return data_dict\n\n\n@dataclass\nclass DataCollatorForSupervisedDataset(object):\n    tokenizer: transformers.PreTrainedTokenizer\n    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n        input_ids = torch.nn.utils.rnn.pad_sequence(\n            input_ids,\n            batch_first=True,\n            padding_value=self.tokenizer.pad_token_id)\n        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n        return dict(\n            input_ids=input_ids,\n            labels=labels,\n            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n        )\n\n# %%\ntrain_dataset = InstructDataset(train_dataset, tokenizer)\nval_dataset = InstructDataset(val_dataset, tokenizer)\ndata_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\nprint(\"cell finished\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\nsample_data = train_dataset[9]\nIGNORE_INDEX=-100\n\nprint(\"=\" * 80)\nprint(\"Debuging: \")\nprint(f\"Input_ids\\n{sample_data['input_ids']}\")\nprint(f\"Label_ids\\n{sample_data['labels']}\")\nprint(\"-\" * 80)\nprint(f\"Input:\\n{tokenizer.decode(sample_data['input_ids'])}\")\nprint(\"-\" * 80)\nN_id = tokenizer.encode(\"N\", add_special_tokens= False)[0]\nprint(f\"Label:\\n{tokenizer.decode([N_id if x == -100 else x for x in sample_data['labels']])}\")\nprint(\"=\" * 80)\n\n\n# %% [markdown]\n# ## Training\n\n# %% [markdown]\n# ### General Training Hyperparameters\n\n# %%\n# Set training parameters\ntraining_arguments = transformers.TrainingArguments(\n    output_dir=\"./checkpoints\" ,\n    # resume_from_checkpoint = \"./checkpoints-1\",\n    num_train_epochs=1,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=2,\n    optim='paged_adamw_32bit',\n    save_steps=0,\n    logging_steps=1,\n    learning_rate=2e-7,\n    weight_decay=0.001,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    dataloader_drop_last=True\n)\n\n# %%\n\nmodel.train()\ntrainer = transformers.Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\ntrainer.train()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\nmodel.print_trainable_parameters()\n\n# %% [markdown]\n# Once the training is completed, we can evaluate our model and get its perplexity on the validation set like this:\n\n# %%\nimport math\n# !pip install -q -U git+https://github.com/huggingface/accelerate.git\neval_results = trainer.evaluate()\nprint(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n\n\n# %% [markdown]\n# ## Save Trained LoRA","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\n!pwd\noutput_path = \"ilora\"\ntrainer.save_model(output_path)\n\n# %% [markdown]\n# ### Test the trained model\n\n# %%\ntemplate = Template(tokenizer.chat_template)\n@torch.no_grad()\ndef generate(prompt):\n    modelInput = template.render(messages=[{\"role\": \"user\", \"content\": prompt}],bos_token= tokenizer.bos_token,add_generation_prompt=True)\n    input_ids = tokenizer.encode(modelInput, add_special_tokens=False, return_tensors='pt').to(\"cuda:0\")\n    outputs = model.generate(input_ids, temperature=1.0, max_new_tokens = 500)\n    model_return_string = tokenizer.decode(*outputs, skip_special_tokens=False)\n    print(\"-\"*80)\n    print(f\"model_return_string:\\n{model_return_string}\")\n    generated_ids = outputs[:, input_ids.shape[1]:]\n    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n    return generated_text\n\nquery = \"我被打了，我可以寻求哪些法律援助？\"\nprint(f\"query:\\n{query}\")\nresponse = generate(query)\nprint(\"-\"*80)\nprint(f\"response:\\n{response}\")\n\n# %% [markdown]\n# # Clean GPU Memory\n\n# %%\n!nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\n# Empty VRAM\n# del model\n# del trainer\nimport gc\nimport torch\ntorch.cuda.empty_cache()\ngc.collect()\ngc.collect()\n\n# %%\n!nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # %% [markdown]\n# # ## Load the trained model back and integrate the trained LoRA within.\n\n# # %%\n\n# from jinja2 import Template\n# template = Template(tokenizer.chat_template)\n# message = \"Please introduce yourself\"\n# print(f\"message:\\n{message}\\n\")\n# message_send_to_model=template.render(messages=[{\"role\": \"user\", \"content\": message}],bos_token=tokenizer.bos_token,add_generation_prompt=True)\n# print(f\"message_send_to_model:\\n{message_send_to_model}\")\n\n# # %%\n# template = Template(tokenizer.chat_template)\n\n# from peft import PeftModel\n# output_path = \"ilora\"\n# IGNORE_INDEX=-100\n\n# # model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map={\"\":0})\n# model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto')\n# model = PeftModel.from_pretrained(model, output_path, is_trainable = True)\n# model = model.merge_and_unload()\n# model.config.max_length = 512\n# # model.eval()\n\n# tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n# # tokenizer.pad_token = tokenizer.unk_token\n\n\n# # ======================== resume from checkpoint ==============================\n# from peft import prepare_model_for_kbit_training\n\n# model.gradient_checkpointing_enable()\n# model = prepare_model_for_kbit_training(model)\n\n# # %%\n# from peft import LoraConfig, get_peft_model\n\n# # You can try differnt parameter-effient strategy for model trianing, for more info, please check https://github.com/huggingface/peft\n# config = LoraConfig(\n#     r=8,\n#     lora_alpha=8,\n#     lora_dropout=0.05,\n#     bias=\"none\",\n#     task_type=\"CAUSAL_LM\",\n# )\n\n# model = get_peft_model(model, config)\n# print(\"cell finished\")\n\n\n# training_arguments = transformers.TrainingArguments(\n#     output_dir=\"./checkpoints_phase2\" ,\n#     resume_from_checkpoint = True,\n#     num_train_epochs=1,\n#     per_device_train_batch_size=2,\n#     per_device_eval_batch_size=2,\n#     gradient_accumulation_steps=2,\n#     optim='paged_adamw_32bit',\n#     save_steps=0,\n#     logging_steps=1,\n#     learning_rate=2e-7,\n#     weight_decay=0.001,\n#     max_steps=-1,\n#     warmup_ratio=0.03,\n#     group_by_length=True,\n#     lr_scheduler_type=\"cosine\",\n#     gradient_checkpointing=True,\n#     report_to=\"none\",\n#     dataloader_drop_last=True\n# )\n\n# # %%\n\n# model.train()\n# trainer = transformers.Trainer(\n#     model=model,\n#     tokenizer=tokenizer,\n#     args=training_arguments,\n#     train_dataset=train_dataset,\n#     eval_dataset=val_dataset,\n#     data_collator=data_collator\n# )\n# trainer.train()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [markdown]\n# ## Answer generation\n\n# %%\n@torch.no_grad()\ndef generate(prompts):\n    model_inputs = [template.render(messages=[{\"role\": \"user\", \"content\": prompt}], bos_token=tokenizer.bos_token, add_generation_prompt=True) for prompt in prompts]\n    input_ids = tokenizer(model_inputs, add_special_tokens=False, return_tensors='pt', padding=True).to(\"cuda:0\")\n\n    outputs = model.generate(input_ids.input_ids,attention_mask=input_ids.attention_mask, max_new_tokens=500)\n\n    generated_texts = []\n    for i in range(len(prompts)):\n        generated_ids = outputs[i, input_ids.input_ids.shape[1]:]\n        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        generated_texts.append(generated_text)\n\n    return generated_texts\n\n# test\nprint(\"\\n\\n\".join(generate([\"I get hit\", \"和妻子发生离婚财产纠纷，属于什么案件？刑事还是民事？\"])))\n\n\n# %%\n!wget https://NLP-course-cuhksz.github.io/Assignments/Assignment1/task1/data/1.exam.json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\nimport json\n\nwith open('1.exam.json') as f:\n  data = json.load(f)\n  data = data[:20] # just for demo\n\nprint(data[0])\n\n# %%\nyour_prompt = \"\"\"请回答下面的多选题，你需要先给出一段解答分析，然后再按照下面的格式样例给出正确答案，格式样例：[答案] B\n{question}\n{options}\"\"\"\n\ndef get_query(da):\n  da['options'] = '\\n'.join([f\"{k}:{v}\" for k, v in da['option'].items() if v])\n  return your_prompt.format_map(da)\n\nfor item in data:\n  item['query'] = get_query(item)\n\n\nprint(data[0]['query'])\n\n# %%\nmodel_answers = generate([item['query'] for item in data])\nprint(f'\\n{model_answers[0]}')\n\n# %%\nimport re\nfrom tqdm import tqdm\n\ndef get_ans(ans):\n    match = re.findall(r'.*?([A-E]+(?:[、, ]+[A-E]+)*)', ans)\n    if match:\n        last_match = match[-1]\n        return ''.join(re.split(r'[、, ，]+', last_match))\n    return ''\n\ncorrect_num = 0\ntotal_num = 0\nfor model_answer, item in tqdm(zip(model_answers, data)):\n  if get_ans(model_answer) == item['answer']:\n    correct_num += 1\n  total_num += 1\n  item['model_answer'] = model_answer\n\nprint(f\"ACC: {correct_num/total_num:.2%}\")\n\nresult_path = \"/content/result.jso\"\nwith open(result_path, \"w\", encoding=\"utf-8\") as file:\n    json.dump(data, file, ensure_ascii=False, indent=4)\n    print(f\"Results are save in {result_path}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-04-18T07:29:00.414Z"}},"outputs":[],"execution_count":null}]}